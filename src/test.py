# Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª
from transformers import pipeline
from langchain.llms import HuggingFacePipeline
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain.chains import RetrievalQA

import os

# 1. ØªØ­Ù…ÙŠÙ„ Ù…Ù„Ù PDF
loader = PyPDFLoader("cv.pdf")  # Ø¶Ø¹ Ø§Ø³Ù… Ù…Ù„ÙÙƒ Ù‡Ù†Ø§
documents = loader.load()

# 2. ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ù…Ù‚Ø§Ø·Ø¹ ØµØºÙŠØ±Ø©
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500, chunk_overlap=50)
docs = text_splitter.split_documents(documents)

# 3. Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ…Ø«ÙŠÙ„Ø§Øª Ø§Ù„Ø±Ù‚Ù…ÙŠØ© (Embeddings) Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¬Ø§Ù†ÙŠ
embedding_model = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2")

# 4. Ø¨Ù†Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª FAISS
vectorstore = FAISS.from_documents(docs, embedding_model)

# 5. Ø§Ø³ØªØ®Ø¯Ø§Ù… LLM Ù…Ø¬Ø§Ù†ÙŠ Ù…Ù† HuggingFace
# Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø³ÙŠØ· Ø¹Ø¨Ø± HuggingFaceHub (ØªØ­ØªØ§Ø¬ Ø­Ø³Ø§Ø¨ Ù…Ø¬Ø§Ù†ÙŠ HuggingFace Token) Ø£Ùˆ Ù†Ø­Ù…Ù„ Ù…ÙˆØ¯ÙŠÙ„ Ù…Ø­Ù„ÙŠ.
# ÙˆÙ„ÙƒÙ† Ù‡Ù†Ø§ Ø³Ø£Ø¹ØªÙ…Ø¯ Ù…ÙˆØ¯ÙŠÙ„ Ø¨Ø³ÙŠØ· Ù…Ø­Ù„ÙŠ Ø¹Ø¨Ø± langchain.llms


# Ø¥Ù†Ø´Ø§Ø¡ Pipeline Ù…Ø­Ù„ÙŠ Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©
qa_pipeline = pipeline(
    "text-generation",
    model="gpt2",     # Ø£Ùˆ ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø®ØªÙŠØ§Ø± Ù†Ù…ÙˆØ°Ø¬ Ø£ÙƒØ¨Ø± Ù„Ùˆ Ø£Ø±Ø¯Øª
    max_new_tokens=200,
    temperature=0.5
)

llm = HuggingFacePipeline(pipeline=qa_pipeline)

# 6. Ø¨Ù†Ø§Ø¡ Ø³Ù„Ø³Ù„Ø© Ø§Ø³ØªØ±Ø¬Ø§Ø¹ ÙˆØ¥Ø¬Ø§Ø¨Ø©
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=vectorstore.as_retriever(),
    chain_type="stuff"
)

# 7. ØªØ¬Ø±Ø¨Ø© Ø§Ù„Ù†Ø¸Ø§Ù…
while True:
    query = input("\nğŸ“ Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ (Ø£Ùˆ Ø§ÙƒØªØ¨ 'Ø®Ø±ÙˆØ¬' Ù„Ù„Ø®Ø±ÙˆØ¬): ")
    if query.lower() == "Ø®Ø±ÙˆØ¬":
        break
    result = qa_chain.run(query)
    print(f"\nâœ… Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©: {result}\n")
